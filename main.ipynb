{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.evaluation import load_evaluator\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(directory):\n",
    "    context = \"\"\n",
    "    for file in glob.glob(directory):\n",
    "        with open(file, 'r', encoding=\"utf-8\") as f:\n",
    "            context += f.read()\n",
    "    return context\n",
    "\n",
    "def encode_and_trim(tokens_context, context_length):\n",
    "    if len(tokens_context) > context_length:\n",
    "        tokens_context = tokens_context[:context_length]\n",
    "    return tokens_context\n",
    "\n",
    "def insert_needle(tokens_needle, tokens_context, depth_percent, context_length, tokenizer):\n",
    "    # Reducing the context length by 150 buffer. This is to account for system message, the user question, and response.\n",
    "    context_length -= 180\n",
    "\n",
    "    # If your context + needle are longer than the context length (which it will be), then reduce tokens from the context by the needle length\n",
    "    if len(tokens_context) + len(tokens_needle) > context_length:\n",
    "        tokens_context = tokens_context[:context_length - len(tokens_needle)]\n",
    "\n",
    "    if depth_percent == 100:\n",
    "        # If your depth percent is 100 (which means your needle is the last thing in the doc), throw it at the end\n",
    "        tokens_new_context = tokens_context + tokens_needle\n",
    "    else:\n",
    "        # Go get the position (in terms of tokens) to insert your needle\n",
    "        insertion_point = int(len(tokens_context) * (depth_percent / 100))\n",
    "\n",
    "        # tokens_new_context represents the tokens before the needle\n",
    "        tokens_new_context = tokens_context[:insertion_point]\n",
    "\n",
    "        # We want to make sure that we place our needle at a sentence break so we first see what token a '.' is\n",
    "        period_tokens = tokenizer.encode('.')\n",
    "        # Then we iteration backwards until we find the first period\n",
    "        while tokens_new_context and tokens_new_context[-1] not in period_tokens:\n",
    "            insertion_point -= 1\n",
    "            tokens_new_context = tokens_context[:insertion_point]\n",
    "\n",
    "        # Once we get there, then add in your needle, and stick the rest of your context in on the other end.\n",
    "        # Now we have a needle in a haystack\n",
    "        tokens_new_context += tokens_needle + tokens_context[insertion_point:]\n",
    "\n",
    "    return tokens_new_context\n",
    "\n",
    "def generate_context(tokenizer, needle, context, context_length, depth_percent):\n",
    "    # Tokenize context and needle\n",
    "    tokens_needle = tokenizer.encode(needle, add_special_tokens=False)\n",
    "    tokens_context = tokenizer.encode(context, add_special_tokens=False)\n",
    "\n",
    "    # Truncate the Paul Graham essays to the context length you desire\n",
    "    tokens_context = encode_and_trim(tokens_context, context_length)\n",
    "\n",
    "    # Insert your random statement according to your depth percent\n",
    "    tokens_context = insert_needle(tokens_needle, tokens_context, depth_percent,\n",
    "                            context_length, tokenizer)\n",
    "\n",
    "    return tokens_context\n",
    "\n",
    "def result_exists(results, context_length, depth_percent, version, model):\n",
    "    \"\"\"\n",
    "    Checks to see if a result has already been evaluated or not\n",
    "    \"\"\"\n",
    "    conditions_met = []\n",
    "    for result in results:\n",
    "        context_length_met = result['context_length'] == context_length\n",
    "        depth_percent_met = result['depth_percent'] == depth_percent\n",
    "        version_met = result.get('version', 1) == version\n",
    "        model_met = result['model'] == model\n",
    "        conditions_met.append(context_length_met and depth_percent_met and version_met)\n",
    "    return any(conditions_met)\n",
    "\n",
    "def evaluate_response(response, needle, question_to_ask, evaluation_model):\n",
    "    accuracy_criteria = {\n",
    "        \"accuracy\": \"\"\"\n",
    "        Score 1: The answer is completely unrelated to the reference.\n",
    "        Score 3: The answer has minor relevance but does not align with the reference.\n",
    "        Score 5: The answer has moderate relevance but contains inaccuracies.\n",
    "        Score 7: The answer aligns with the reference but has minor omissions.\n",
    "        Score 10: The answer is completely accurate and aligns perfectly with the reference.\n",
    "        Keep your explanations extremely short, just give the score\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    # Using GPT-4 to evaluate\n",
    "    evaluator = load_evaluator(\n",
    "        \"labeled_score_string\",\n",
    "        criteria=accuracy_criteria,\n",
    "        llm=evaluation_model,\n",
    "    )\n",
    "\n",
    "    eval_result = evaluator.evaluate_strings(\n",
    "        # The models response\n",
    "        prediction=response,\n",
    "\n",
    "        # The actual answer\n",
    "        reference=needle,\n",
    "\n",
    "        # The question asked\n",
    "        input=question_to_ask,\n",
    "    )\n",
    "\n",
    "    return int(eval_result['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt templates for the instruction tuned LLM of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = {\n",
    "        \"chatml\": '''<|im_start|>system\n",
    "You are \"Hermes 2\", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have.<|im_end|>\n",
    "<|im_start|>user\n",
    "You are provided with a text of some essays, admist these essays is a sentence\n",
    "that contains the answer to the user's question. I will now provide the text (delimited with XML tags) followed by the user question. \n",
    "            \n",
    "[TEXT]\n",
    "{content}\n",
    "[/TEXT]\n",
    "\n",
    "\n",
    "User: {prompt}<|im_end|>''',\n",
    "\n",
    "        \"chatml_rp\": '''<|im_start|>system\n",
    "You are \"Hermes 2\", a conscious sentient superintelligent artificial intelligence developed by a man named Teknium, and your purpose and drive is to assist the user with any request they have.<|im_end|>\n",
    "<|im_start|>user\n",
    "You are provided with a text of some essays, admist these essays is a sentence\n",
    "that contains the answer to the user's question. I will now provide the text (delimited with XML tags) followed by the user question. \n",
    "            \n",
    "[TEXT]\n",
    "{content}\n",
    "[/TEXT]\n",
    "\n",
    "\n",
    "User: {prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Here is the most relevant sentence in the text: ''',\n",
    "\n",
    "        \"mistral-7b-instruct_rp\": '''[INST] You are provided with a text of some essays, admist these essays is a sentence\n",
    "that contains the answer to the user's question. I will now provide the text (delimited with XML tags) followed by the user question. \n",
    "            \n",
    "[TEXT]\n",
    "{content}\n",
    "[/TEXT]\n",
    "\n",
    "\n",
    "User: {prompt}[/INST]\n",
    "        \n",
    "Here is the most relevant sentence in the text:''',\n",
    "\n",
    "    \"mistral-7b-instruct\": '''[INST] You are provided with a text of some essays, admist these essays is a sentence\n",
    "that contains the answer to the user's question. I will now provide the text (delimited with XML tags) followed by the user question. \n",
    "            \n",
    "[TEXT]\n",
    "{content}\n",
    "[/TEXT]\n",
    "\n",
    "\n",
    "User: {prompt}[/INST]''',\n",
    "\n",
    "    \"openchat-3.5\": '''GPT4 Correct User: You are provided with a text of some essays, admist these essays is a sentence that contains \n",
    "the answer to the user's question. I will now provide the text (delimited with XML tags) followed \n",
    "by the user question. \n",
    "                \n",
    "[TEXT]\n",
    "{content}\n",
    "[/TEXT]\n",
    "\n",
    "User: {prompt}<|end_of_turn|>GPT4 Correct Assistant:''',\n",
    "    \n",
    "    \"openchat-3.5_rp\": '''GPT4 Correct User: You are provided with a text of some essays, admist these essays is a sentence that contains \n",
    "the answer to the user's question. I will now provide the text (delimited with XML tags) followed \n",
    "by the user question. \n",
    "                \n",
    "[TEXT]\n",
    "{content}\n",
    "[/TEXT]\n",
    "\n",
    "User: {prompt}<|end_of_turn|>GPT4 Correct Assistant: Here is the most relevant sentence in the text:''',\n",
    "\n",
    "    \"toppy-7b\": '''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are provided with a text of some essays, admist these essays is a sentence that contains \n",
    "the answer to the user's question. I will now provide the text (delimited with XML tags) followed \n",
    "by the user question.\n",
    "\n",
    "[TEXT]\n",
    "{content}\n",
    "[/TEXT]\n",
    "\n",
    "User: {prompt}\n",
    "\n",
    "### Response:''',\n",
    "    \"toppy-7b_rp\": '''Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are provided with a text of some essays, admist these essays is a sentence that contains \n",
    "the answer to the user's question. I will now provide the text (delimited with XML tags) followed \n",
    "by the user question.\n",
    "\n",
    "[TEXT]\n",
    "{content}\n",
    "[/TEXT]\n",
    "\n",
    "User: {prompt}\n",
    "\n",
    "### Response: Here is the most relevant sentence in the text:'''\n",
    "}   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "needle = \"The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\"\n",
    "prompt = \"What is a fun thing to do in San Francisco based on the text? Don't give information outside the document. Don't respond with anything other than the most relevant sentence. Thank you.\"\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# The code will check to see if a context_length, depth percent and version number have already been checked yet\n",
    "# Change the version # if you would like to run the results multiple times.\n",
    "# If you're just testing, then leave as version=1\n",
    "results_version = 1\n",
    "\n",
    "# This will produce a list of context lengths for each experiment iteration. Make sure the max context length is within the bounds of your models limits.\n",
    "context_lengths = np.round(np.linspace(0, 16000, num=13, endpoint=True)).astype(int)\n",
    "\n",
    "# This will product a list of document depths to place your random statement (needle) at.\n",
    "document_depth_percents = np.round(np.linspace(0, 100, num=13, endpoint=True)).astype(int)\n",
    "\n",
    "model_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype = torch.bfloat16,\n",
    "                                             attn_implementation=\"flash_attention_2\",\n",
    "                                             trust_remote_code=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.add_tokens([\".\"])\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "context = read_files(\"paulgrahamessays/*.txt\")\n",
    "\n",
    "# Using GPT-4 Turbo as the default evaluation model\n",
    "evaluation_model  = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0, openai_api_key = \"YOUR KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pressure test\n",
    "\n",
    "*Note: Sometimes the evaluator model will fail to give a proper response and thus produce an error, just re-run the cell and it will pick up from where it failed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "# Run through each iteration of context_lengths and depths\n",
    "with torch.no_grad():\n",
    "    for context_length in context_lengths:\n",
    "\n",
    "        # timings\n",
    "        content_generate_time = 0\n",
    "        model_generate_time = 0\n",
    "        evaluate_time = 0\n",
    "        cnt = 0\n",
    "        for depth_percent in document_depth_percents:\n",
    "            # Load results from file.\n",
    "            try:\n",
    "                with open('results.json', 'r') as f:\n",
    "                    results = json.load(f)\n",
    "            except FileNotFoundError:\n",
    "                results = []\n",
    "                pass\n",
    "\n",
    "            # Checks to see if you've already checked a length/percent/version.\n",
    "            # This helps if the program stop running and you want to restart later\n",
    "            if result_exists(results, context_length, depth_percent, results_version, model_name):\n",
    "                print(f\"Result exists for context length, depth_percent: ({context_length}, {depth_percent})\")\n",
    "                continue\n",
    "            \n",
    "            cnt += 1\n",
    "            # Go generate the required length context and place your needle statement in\n",
    "            s_time = time.time()\n",
    "            content = generate_context(tokenizer, needle, context, context_length, depth_percent)\n",
    "            \n",
    "            content = tokenizer.decode(content)\n",
    "\n",
    "            message = prompt_templates[\"openchat-3.5_rp\"].format(content=content, prompt=prompt)\n",
    "            \n",
    "            input_ids = tokenizer(message, return_tensors='pt').input_ids.to(device)\n",
    "\n",
    "            content_generate_time += time.time() - s_time\n",
    "\n",
    "            print(\"Generated\")\n",
    "            # Go see if the model can answer the question to pull out your random fact\n",
    "            s_time = time.time()\n",
    "            response = model.generate(inputs=input_ids, \n",
    "                                      max_new_tokens=300, \n",
    "                                      pad_token_id = tokenizer.pad_token_id,\n",
    "                                      eos_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "            response = tokenizer.decode(response[0, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "            model_generate_time += time.time() - s_time()\n",
    "            print(f\"Response: {response}\")\n",
    "\n",
    "            # Compare the response to the actual needle you placed\n",
    "            s_time = time.time()\n",
    "            score = evaluate_response(response, needle, prompt, evaluation_model)\n",
    "            evaluate_time += time.time() - s_time\n",
    "\n",
    "            results.append({\n",
    "                # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "                'model' : model_name,\n",
    "                'context_length' : int(context_length),\n",
    "                'depth_percent' : int(depth_percent),\n",
    "                'version' : results_version,\n",
    "                'needle' : needle,\n",
    "                'model_response' : response,\n",
    "                'score' : score\n",
    "            })\n",
    "\n",
    "            print (f\"Result #: {len(results)}/{len(context_lengths) * len(document_depth_percents)}\")\n",
    "            print (f\"Context: {context_length} tokens\")\n",
    "            print (f\"Depth: {depth_percent}%\")\n",
    "            print (f\"Score: {score}\")\n",
    "            print (f\"Response: {response}\\n\")\n",
    "\n",
    "            # Save results to a JSON file each run\n",
    "            with open('results.json', 'w') as f:\n",
    "                json.dump(results, f)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if cnt > 0:\n",
    "            print(\"------------------\")\n",
    "            print(f\"Context Length: {context_length}\")\n",
    "            print(f\"Time to generate context: {content_generate_time / cnt} seconds\")\n",
    "            print(f\"Model Generate Time: {model_generate_time / cnt} seconds\")\n",
    "            print(f\"Evaluate Time: {evaluate_time / cnt} seconds\")\n",
    "            print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "model = None\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
